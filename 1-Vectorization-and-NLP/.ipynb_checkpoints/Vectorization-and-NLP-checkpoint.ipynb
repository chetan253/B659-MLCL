{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCL Assignment 1 : Submitted by Chetan Patil\n",
    "## Vectorization and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: CoreNLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pos](pictures/pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ner](pictures/ner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lemma](pictures/lemma.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dependency](pictures/dep.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![constituency](pictures/const.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![coref](pictures/coref.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Strategy :\n",
    "Let us consider a particular task over which the above given features will be useful for us in the study of NLP-analysis\n",
    "\n",
    "**Task:** Document / Author Classification\n",
    "\n",
    "**Part-of-speech :**\n",
    "The part-of-speech property gives us a grammer tag (i.e nouns, verbs, adjectives etc.) associated with every word in the sentence which can be used in our task in order to determine the correct phrases as well as fitting of a particular word in a suitable context.\n",
    "We can map the number of occurences of a POS tag and include that in a feature vector:\n",
    "\n",
    "|Noun|Verb|Adverb|Adjective|\n",
    "|----|----|------|---------|\n",
    "|5   |2   |0     |1        |\n",
    "\n",
    "**Named entities :**\n",
    "Named entities gives us a class/category for which a paricular word belongs to. Similar, to that of POS tags we can take total number of occurences and map them in the feature vector\n",
    "\n",
    "|Person|Date|Location|Organization|\n",
    "|----|----|------|---------|\n",
    "|5   |2   |0     |1        |\n",
    "\n",
    "**Lemma :**\n",
    "Lemmatization can be used to detect if two words have same root or not which is another form of text normalization and can be used in our task for stemming purpose.\n",
    "To include this in feature vector we simply take the ratio of total # of tokens / # of unique lemmas\n",
    "\n",
    "|Lemma|\n",
    "|-----|\n",
    "|1.18|\n",
    "\n",
    "**Coreference :**\n",
    "In our task coreference can be used to link the relationship between names and corresponding tags referred for them in future sentences. We take the count of genders i.e Male, Female, Neutral references from coreference and map them to vector\n",
    "\n",
    "|Male|Female|Neutral|\n",
    "|----|----|------|\n",
    "|1   |1   |0     |\n",
    "\n",
    "**Dependency Parse Tree :**\n",
    "Using dependecy tree we can quantify the usage of passive or active voice by extracting the total number of occurences of auxpass and subjpass and map them in the feature vector\n",
    "\n",
    "|auxpass|nsubjpass|\n",
    "|----|----|\n",
    "|1   |1   |\n",
    "\n",
    "**Constituent Parse Tree:**\n",
    "Contituent parse tree can be used to extract the chunks of data that relate to grammers and related phrases from the tree. We do this by taking the count of leaf nodes for respectie phrases\n",
    "\n",
    "|NP|VP|\n",
    "|----|----|\n",
    "|5   |7   |\n",
    "\n",
    "\n",
    "### Full Feature vector:\n",
    "The full feature vector will consist of all the above given vectors combined in one with the following mapping sequence of properties:\n",
    "[POS tags, Named Entities, Lemma, Coreferences, Dependecy parse tree, constituent parse tree]\n",
    "where each element in above sequence represents the continous flatten numpy vector which in turn can be used to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Spacy NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "sentence = nlp(u'John met Susan in the mall. She told him that she is traveling to Europe next week.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "met\n",
      "Susan\n",
      "in\n",
      "the\n",
      "mall\n",
      ".\n",
      "She\n",
      "told\n",
      "him\n",
      "that\n",
      "she\n",
      "is\n",
      "traveling\n",
      "to\n",
      "Europe\n",
      "next\n",
      "week\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-Speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John john PROPN NNP nsubj\n",
      "met meet VERB VBD ROOT\n",
      "Susan susan PROPN NNP dobj\n",
      "in in ADP IN prep\n",
      "the the DET DT det\n",
      "mall mall NOUN NN pobj\n",
      ". . PUNCT . punct\n",
      "She -PRON- PRON PRP nsubj\n",
      "told tell VERB VBD ROOT\n",
      "him -PRON- PRON PRP dobj\n",
      "that that ADP IN mark\n",
      "she -PRON- PRON PRP nsubj\n",
      "is be VERB VBZ aux\n",
      "traveling travel VERB VBG ccomp\n",
      "to to ADP IN prep\n",
      "Europe europe PROPN NNP pobj\n",
      "next next ADJ JJ amod\n",
      "week week NOUN NN npadvmod\n",
      ". . PUNCT . punct\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities: \n",
    "We can extract named entities from documents using the ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    John\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " met \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Susan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in the mall. She told him that she is traveling to \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Europe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    next week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sentence, style = 'ent', jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John 0 4 PERSON\n",
      "Susan 9 14 PERSON\n",
      "Europe 66 72 LOC\n",
      "next week 73 82 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"3025\" height=\"574.5\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">met</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Susan</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mall.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">told</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">she</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">traveling</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">Europe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">next</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">week.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M245,439.5 C245,264.5 560.0,264.5 560.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,441.5 L568.0,429.5 552.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M595,439.5 C595,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,441.5 L918.0,429.5 902.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1430.0,441.5 L1438.0,429.5 1422.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,177.0 2140.0,177.0 2140.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,441.5 L1637,429.5 1653,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,264.5 2135.0,264.5 2135.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,441.5 L1987,429.5 2003,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,2.0 2150.0,2.0 2150.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2150.0,441.5 L2158.0,429.5 2142.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-11\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2305.0,441.5 L2313.0,429.5 2297.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-12\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,352.0 2480.0,352.0 2480.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-12\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2480.0,441.5 L2488.0,429.5 2472.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-13\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,352.0 2830.0,352.0 2830.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-13\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,441.5 L2687,429.5 2703,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-14\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,89.5 2845.0,89.5 2845.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-14\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2845.0,441.5 L2853.0,429.5 2837.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John nsubj met VERB []\n",
      "met ROOT met VERB [John, Susan, in, .]\n",
      "Susan dobj met VERB []\n",
      "in prep met VERB [mall]\n",
      "the det mall NOUN []\n",
      "mall pobj in ADP [the]\n",
      ". punct met VERB []\n",
      "She nsubj told VERB []\n",
      "told ROOT told VERB [She, him, traveling, .]\n",
      "him dobj told VERB []\n",
      "that mark traveling VERB []\n",
      "she nsubj traveling VERB []\n",
      "is aux traveling VERB []\n",
      "traveling ccomp told VERB [that, she, is, to, week]\n",
      "to prep traveling VERB [Europe]\n",
      "Europe pobj to ADP []\n",
      "next amod week NOUN []\n",
      "week npadvmod traveling VERB [next]\n",
      ". punct told VERB []\n"
     ]
    }
   ],
   "source": [
    "displacy.render(sentence, style = \"dep\", jupyter = True)\n",
    "for token in sentence:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John john\n",
      "met meet\n",
      "Susan susan\n",
      "in in\n",
      "the the\n",
      "mall mall\n",
      ". .\n",
      "She -PRON-\n",
      "told tell\n",
      "him -PRON-\n",
      "that that\n",
      "she -PRON-\n",
      "is be\n",
      "traveling travel\n",
      "to to\n",
      "Europe europe\n",
      "next next\n",
      "week week\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Strategy :\n",
    " \n",
    "**Note :** The vectorization strategy for both CoreNLP and Spacy will be same just the Final Feature vector will not contain the subvectors related to constituent parse tree and coreferences as functionality for them is not supported by spacy\n",
    "\n",
    "### Comparision:\n",
    "Using Spacy can help us to find the integrated word vectors which will help us in the task to define the similarity between the document text. While this feature is absent in CoreNLP pipeline\n",
    "\n",
    "The difference in the outputs can be determined in above given sentence at the word \"next\" while checking POS tags spacy tags the word as ADJ while CoreNLP portrays that as IN. This might depend upon the different corpora used by them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 :\n",
    "\n",
    "#### Note: Considered the counts for limited labels for POS, NER, Dependencies based on above sentence as per question. We can add the count for all of tags which are not present and assign them to 0 which will be sparse vector. \n",
    "\n",
    "### Part 1: Taking JSON from server and mapping to Python data structure (CoreNLP)\n",
    "This part uses the pywrap as the wrapper to map required data structures to dictionary of lists associated with respective properties except dependencies and constituent parse trees which are mapped to lists using JSON in the form of dictionary traversing.\n",
    "In order to parse constituent tree and get NP and VP values the tree is traversed by passing it to nltk tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corenlp_pywrap import pywrap\n",
    "from nltk.tree import Tree\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens\n",
      " ['John', 'met', 'Susan', 'in', 'the', 'mall', '.', 'She', 'told', 'him', 'that', 'she', 'is', 'traveling', 'to', 'Europe', 'next', 'week', '.'] \n",
      "\n",
      "Parts-of-Speech ['NNP', 'VBD', 'NNP', 'IN', 'DT', 'NN', '.', 'PRP', 'VBD', 'PRP', 'IN', 'PRP', 'VBZ', 'VBG', 'TO', 'NNP', 'IN', 'NN', '.'] \n",
      "\n",
      "Named Entities\n",
      " ['PERSON', 'O', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOCATION', 'DATE', 'DATE', 'O'] \n",
      "\n",
      "Lemma\n",
      " ['John', 'meet', 'Susan', 'in', 'the', 'mall', '.', 'she', 'tell', 'he', 'that', 'she', 'be', 'travel', 'to', 'Europe', 'next', 'week', '.'] \n",
      "\n",
      "Coreferences \n",
      " [('Europe', 'NEUTRAL', 'SINGULAR', 'PROPER'), ('the mall', 'NEUTRAL', 'SINGULAR', 'NOMINAL'), ('Susan', 'FEMALE', 'SINGULAR', 'PROPER'), ('She', 'FEMALE', 'SINGULAR', 'PRONOMINAL'), ('she', 'FEMALE', 'SINGULAR', 'PRONOMINAL'), ('John', 'MALE', 'SINGULAR', 'PROPER'), ('him', 'MALE', 'SINGULAR', 'PRONOMINAL'), ('next week', 'UNKNOWN', 'SINGULAR', 'PROPER')] \n",
      "\n",
      "Dependencies\n",
      " [('met', 'ROOT', 'ROOT'), ('John', 'nsubj', 'met'), ('Susan', 'dobj', 'met'), ('in', 'case', 'mall'), ('the', 'det', 'mall'), ('mall', 'nmod', 'met'), ('.', 'punct', 'met'), ('told', 'ROOT', 'ROOT'), ('She', 'nsubj', 'told'), ('him', 'dobj', 'told'), ('that', 'mark', 'traveling'), ('she', 'nsubj', 'traveling'), ('is', 'aux', 'traveling'), ('traveling', 'ccomp', 'told'), ('to', 'case', 'Europe'), ('Europe', 'nmod', 'traveling'), ('next', 'amod', 'week'), ('week', 'nmod:tmod', 'traveling'), ('.', 'punct', 'told')] \n",
      "\n",
      "Constituent Parse Trees\n",
      "\n",
      "               ROOT                 \n",
      "                |                    \n",
      "                S                   \n",
      "  ______________|_________________   \n",
      " |              VP                | \n",
      " |     _________|____             |  \n",
      " |    |    |         PP           | \n",
      " |    |    |     ____|___         |  \n",
      " NP   |    NP   |        NP       | \n",
      " |    |    |    |     ___|___     |  \n",
      "NNP  VBD  NNP   IN   DT      NN   . \n",
      " |    |    |    |    |       |    |  \n",
      "John met Susan  in  the     mall  . \n",
      "\n",
      "                       ROOT                                           \n",
      "                        |                                              \n",
      "                        S                                             \n",
      "  ______________________|___________________________________________   \n",
      " |        VP                                                        | \n",
      " |    ____|________                                                 |  \n",
      " |   |    |       SBAR                                              | \n",
      " |   |    |    ____|____                                            |  \n",
      " |   |    |   |         S                                           | \n",
      " |   |    |   |     ____|______________                             |  \n",
      " |   |    |   |    |                   VP                           | \n",
      " |   |    |   |    |     ______________|________                    |  \n",
      " |   |    |   |    |    |                       VP                  | \n",
      " |   |    |   |    |    |        _______________|__________         |  \n",
      " |   |    |   |    |    |       |          PP              |        | \n",
      " |   |    |   |    |    |       |       ___|____           |        |  \n",
      " NP  |    NP  |    NP   |       |      |        NP         NP       | \n",
      " |   |    |   |    |    |       |      |        |      ____|___     |  \n",
      "PRP VBD  PRP  IN  PRP  VBZ     VBG     TO      NNP    IN       NN   . \n",
      " |   |    |   |    |    |       |      |        |     |        |    |  \n",
      "She told him that she   is  traveling  to     Europe next     week  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mapper\n",
    "corefs = []\n",
    "gender_count = {} #specific\n",
    "tokens = []\n",
    "pos = []\n",
    "pos_count = {'N':0,'V':0,'J':0,'R':0}  #specific\n",
    "lemma = []\n",
    "ner = []\n",
    "ner_count = {'PERSON': 0, 'DATE': 0, 'LOCATION': 0, 'ORGANIZATION': 0}  #specific\n",
    "dependencies = []\n",
    "dependency_count = {'aux' : 0, 'auxpass': 0, 'nsubjpass' : 0, 'nsubj':0, 'dobj':0} #nsubj dobj added just for check\n",
    "noun_phrase_count = 0\n",
    "verb_phrase_count = 0\n",
    "\n",
    "text = ('John met Susan in the mall. She told him that she is traveling to Europe next week.')\n",
    "full_annotator_list = [\"tokenize\", \"pos\", \"lemma\", \"ner\", \"parse\", \"depparse\", \"dcoref\"]\n",
    "cn = pywrap.CoreNLP(url='http://localhost:9000', annotator_list=full_annotator_list)\n",
    "\n",
    "#Calling basic function which would return a 'requests' object\n",
    "out = cn.basic(text, out_format='json')\n",
    "token_dict = cn.arrange(text)\n",
    "outputjson = out.json()\n",
    "obj = outputjson['corefs']\n",
    "\n",
    "def updateGenderCount(gender):\n",
    "    if gender in gender_count:\n",
    "        gender_count[gender] += 1\n",
    "    else:\n",
    "        gender_count[gender] = 1\n",
    "\n",
    "def updatePOSCount(pos):\n",
    "    if pos[0] in pos_count:\n",
    "        pos_count[pos[0]] += 1\n",
    "\n",
    "def updateNERCount(entity):\n",
    "    if entity in ner_count:\n",
    "        ner_count[entity] += 1\n",
    "\n",
    "#Extracting Coreferences\n",
    "for values in obj:\n",
    "    for values1 in obj[values]:\n",
    "        tup = (values1['text'], values1['gender'], values1['number'], values1['type'])\n",
    "        updateGenderCount(values1['gender'])\n",
    "        corefs.append(tup)\n",
    "#Extracting Parts-of-Speech\n",
    "for pos in token_dict['pos']:\n",
    "    if(pos[0] in ('N','V','J','R')):\n",
    "        updatePOSCount(pos)\n",
    "\n",
    "#Extracting Named Entities\n",
    "for entity in token_dict['ner']:\n",
    "    if(entity not in ('O')):\n",
    "        updateNERCount(entity)\n",
    "\n",
    "#Extracting Dependencies\n",
    "obj = outputjson['sentences']\n",
    "for values in obj:\n",
    "    basicDependencies = values['basicDependencies']\n",
    "    for data in basicDependencies:\n",
    "        tup = (data['dependentGloss'], data['dep'], data['governorGloss'])\n",
    "        dependencies.append(tup)\n",
    "        if data['dep'] in dependency_count.keys():\n",
    "            dependency_count[data['dep']] += 1\n",
    "\n",
    "#Ref: https://www.winwaed.com/blog/2012/01/20/extracting-noun-phrases-from-parsed-trees/\n",
    "def ExtractPhrases( myTree, phrase):\n",
    "    myPhrases = []\n",
    "    if (myTree.label() == phrase):\n",
    "        myPhrases.append( myTree.copy(True) )\n",
    "    for child in myTree:\n",
    "        if (type(child) is Tree):\n",
    "            list_of_phrases = ExtractPhrases(child, phrase)\n",
    "            if (len(list_of_phrases) > 0):\n",
    "                myPhrases.extend(list_of_phrases)\n",
    "    return myPhrases\n",
    "print(\"Tokens\\n\", token_dict[\"word\"],\"\\n\")\n",
    "print(\"Parts-of-Speech\", token_dict[\"pos\"],\"\\n\")\n",
    "print(\"Named Entities\\n\", token_dict[\"ner\"],\"\\n\")\n",
    "print(\"Lemma\\n\", token_dict[\"lemma\"],\"\\n\")\n",
    "print(\"Coreferences \\n\",corefs,\"\\n\")\n",
    "print(\"Dependencies\\n\",dependencies,\"\\n\")\n",
    "print(\"Constituent Parse Trees\\n\")\n",
    "for tree in outputjson['sentences']:\n",
    "    parsetree = Tree.fromstring(tree['parse'])\n",
    "    list_of_noun_phrases = ExtractPhrases(parsetree, 'NP')\n",
    "    list_of_verb_phrases = ExtractPhrases(parsetree, 'VP')\n",
    "    for phrase in list_of_noun_phrases:\n",
    "        noun_phrase_count += 1\n",
    "    for phrase in list_of_verb_phrases:\n",
    "        verb_phrase_count += 1\n",
    "    parsetree.pretty_print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of CoreNLP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vector:\n",
      " [0.         5.         0.         4.         2.         1.\n",
      " 0.         2.         1.11764706 3.         2.         2.\n",
      " 1.         1.         0.         2.         3.         0.\n",
      " 8.         4.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_vector = np.array([pos_count[key] for key in sorted(pos_count)])\n",
    "ner_vector = np.array([ner_count[key] for key in sorted(ner_count)])\n",
    "lemma = np.array([len(token_dict['word'])/float(len(set(token_dict['lemma'])))])\n",
    "coref_gender_vector = np.array([gender_count[key] for key in sorted(gender_count)])\n",
    "dependency_parse_vector = np.array([dependency_count[key] for key in sorted(dependency_count)])\n",
    "noun_phrase_count = np.array([noun_phrase_count])\n",
    "verb_phrase_count = np.array([verb_phrase_count])\n",
    "\n",
    "#sequence\n",
    "feature_vector = [pos_vector, ner_vector, lemma, coref_gender_vector, dependency_parse_vector, noun_phrase_count, verb_phrase_count]\n",
    "f_vector = [val for i in feature_vector for val in i]\n",
    "print(\"Full vector:\\n\",np.array(f_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Spacy NLP-objects to vectors\n",
    "We can similarly make vectors of spacy objects as of that of CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens\n",
      " [John, met, Susan, in, the, mall, ., She, told, him, that, she, is, traveling, to, Europe, next, week, .] \n",
      "\n",
      "Parts-of-Speech\n",
      " [('John', 'PROPN', 'NNP'), ('met', 'VERB', 'VBD'), ('Susan', 'PROPN', 'NNP'), ('in', 'ADP', 'IN'), ('the', 'DET', 'DT'), ('mall', 'NOUN', 'NN'), ('.', 'PUNCT', '.'), ('She', 'PRON', 'PRP'), ('told', 'VERB', 'VBD'), ('him', 'PRON', 'PRP'), ('that', 'ADP', 'IN'), ('she', 'PRON', 'PRP'), ('is', 'VERB', 'VBZ'), ('traveling', 'VERB', 'VBG'), ('to', 'ADP', 'IN'), ('Europe', 'PROPN', 'NNP'), ('next', 'ADJ', 'JJ'), ('week', 'NOUN', 'NN'), ('.', 'PUNCT', '.')] \n",
      "\n",
      "Named Entities\n",
      " [('John', 'PERSON'), ('Susan', 'PERSON'), ('Europe', 'LOC'), ('next week', 'DATE')] \n",
      "\n",
      "Lemma\n",
      " ['john', 'meet', 'susan', 'in', 'the', 'mall', '.', '-PRON-', 'tell', '-PRON-', 'that', '-PRON-', 'be', 'travel', 'to', 'europe', 'next', 'week', '.'] \n",
      "\n",
      "Dependencies\n",
      " [('John', 'nsubj', 'met'), ('met', 'ROOT', 'met'), ('Susan', 'dobj', 'met'), ('in', 'prep', 'met'), ('the', 'det', 'mall'), ('mall', 'pobj', 'in'), ('.', 'punct', 'met'), ('She', 'nsubj', 'told'), ('told', 'ROOT', 'told'), ('him', 'dobj', 'told'), ('that', 'mark', 'traveling'), ('she', 'nsubj', 'traveling'), ('is', 'aux', 'traveling'), ('traveling', 'ccomp', 'told'), ('to', 'prep', 'traveling'), ('Europe', 'pobj', 'to'), ('next', 'amod', 'week'), ('week', 'npadvmod', 'traveling'), ('.', 'punct', 'told')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sentence = nlp(u'John met Susan in the mall. She told him that she is traveling to Europe next week.')\n",
    "# mappers\n",
    "sp_tokens = []\n",
    "sp_lemma = []\n",
    "sp_dependencies = []\n",
    "sp_dependency_count = {'aux': 0 , 'auxpass': 0, 'nsubjpass' : 0, 'nsubj':0, 'dobj':0}\n",
    "sp_ner = []\n",
    "sp_ner_count = {'PERSON': 0, 'DATE': 0, 'LOCATION': 0, 'ORGANIZATION': 0}  #specific\n",
    "sp_pos = []\n",
    "sp_pos_count = {'N':0,'V':0,'J':0,'R':0}\n",
    "\n",
    "#counters for vectors\n",
    "def spUpdatePOSCount(pos):\n",
    "    if pos[0] in sp_pos_count:\n",
    "        sp_pos_count[pos[0]] += 1    \n",
    "\n",
    "def spUpdateNERCount(entity):\n",
    "    if entity in sp_ner_count:\n",
    "        sp_ner_count[entity] += 1\n",
    "        \n",
    "def spUpdateDependencyCount(dependency):\n",
    "    if dependency in sp_dependency_count:\n",
    "        sp_dependency_count[dependency] += 1\n",
    "\n",
    "#Extracting tokens, lemmas, dependencies, parts-of-speech\n",
    "for token in sentence:\n",
    "    #lemma\n",
    "    sp_tokens.append(token)\n",
    "    sp_lemma.append(token.lemma_)\n",
    "    #dependencies\n",
    "    tup_dependencies = (token.text, token.dep_, token.head.text)\n",
    "    sp_dependencies.append(tup_dependencies)\n",
    "    if(token.dep_ in sp_dependency_count.keys()):\n",
    "        spUpdateDependencyCount(token.dep_)\n",
    "    #parts-of-speech\n",
    "    tup_pos = (token.text, token.pos_, token.tag_)\n",
    "    sp_pos.append(tup_pos)\n",
    "    if(token.tag_[0] in sp_pos_count.keys()):\n",
    "        spUpdatePOSCount(token.tag_)\n",
    "    \n",
    "#Extracting named entities\n",
    "for ent in sentence.ents:\n",
    "    tup_ent = (ent.text, ent.label_)\n",
    "    sp_ner.append(tup_ent)\n",
    "    if ent.label_ in sp_ner_count.keys():\n",
    "        spUpdateNERCount(ent.label_)\n",
    "\n",
    "print(\"Tokens\\n\", sp_tokens,\"\\n\")\n",
    "print(\"Parts-of-Speech\\n\", sp_pos,\"\\n\")\n",
    "print(\"Named Entities\\n\", sp_ner,\"\\n\")\n",
    "print(\"Lemma\\n\", sp_lemma,\"\\n\")\n",
    "print(\"Dependencies\\n\",sp_dependencies,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of Spacy NLP-Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vector:\n",
      " [1.     5.     0.     4.     1.     0.     0.     2.     1.1875 1.\n",
      " 0.     2.     3.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "sp_pos_vector = np.array([sp_pos_count[key] for key in sorted(sp_pos_count)])\n",
    "sp_ner_vector = np.array([sp_ner_count[key] for key in sorted(sp_ner_count)])\n",
    "sp_lemma_vector = np.array([len(sp_tokens)/float(len(set(sp_lemma)))])\n",
    "sp_dependency_vector = np.array([sp_dependency_count[key] for key in sorted(sp_dependency_count)])\n",
    "\n",
    "sp_feature_vector = [sp_pos_vector, sp_ner_vector, sp_lemma_vector, sp_dependency_vector]\n",
    "sp_f_vector = [val for i in sp_feature_vector for val in i]\n",
    "sp_f_vector = np.array(sp_f_vector)\n",
    "print(\"Full vector:\\n\",sp_f_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Text Classification based on Spacy and fasttext vectors and embeddings\n",
    "\n",
    "Used fasttext and spacy python libraries for text classification for 2 classes i.e Films and Politics which I created using the Wikipedia from the websites (https://en.wikipedia.org/wiki/Politics) for Politics and (https://en.wikipedia.org/wiki/Film) for Films. The data contains total of 100 sentences 50 for fils and 50 for politics class. I used supervised learning model to train the data which uses 'model_filmpolitics.bin'. The \n",
    "model was tested on 5 sentences and respective labels were predicted.\n",
    "\n",
    "The 'model_filmpolitics.vec' is takend as input vector in spacy in order to get similarity between tokens based on our corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "import plac\n",
    "import numpy\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['__label__politics'], ['__label__film'], ['__label__politics'], ['__label__film'], ['__label__politics']]\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.supervised('filmpolitics.txt', 'model_filmpolitics')\n",
    "\n",
    "classifier = fasttext.load_model('model_filmpolitics.bin')\n",
    "\n",
    "#prediction text\n",
    "text = ['House Minority Leader Nancy Pelosi (D-Calif.) has earned the ire of Republicans for suggesting that major corporations are giving workers “crumbs” while top executives reap bonuses after passage of the GOP’s tax revision plan.',\n",
    "'The 10 Most Anticipated Movie Trailers Of 2018',\n",
    "'Going into \"Avengers: Infinity War,\" only one of the six Infinity Stones remains unaccounted for - the Soul Stone.',\n",
    "'The White House Denies National Security Adviser H.R. McMaster Is Departing',\n",
    "'President Trump told business executives gathered at the White House Thursday that he would impose tariffs on steel and aluminum next week.']\n",
    "\n",
    "\n",
    "labels = classifier.predict(text)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The The 1.0\n",
      "The White 0.0\n",
      "The House 0.0\n",
      "The Denies 0.0\n",
      "The National 0.0\n",
      "The Security 0.0\n",
      "The Adviser 0.0\n",
      "The H.R. 0.0\n",
      "The McMaster 0.0\n",
      "The Is 0.0\n",
      "The Departing 0.0\n",
      "White The 0.0\n",
      "White White 1.0\n",
      "White House 0.0\n",
      "White Denies 0.0\n",
      "White National 0.0\n",
      "White Security 0.0\n",
      "White Adviser 0.0\n",
      "White H.R. 0.0\n",
      "White McMaster 0.0\n",
      "White Is 0.0\n",
      "White Departing 0.0\n",
      "House The 0.0\n",
      "House White 0.0\n",
      "House House 1.0\n",
      "House Denies 0.0\n",
      "House National 0.0\n",
      "House Security 0.0\n",
      "House Adviser 0.0\n",
      "House H.R. 0.0\n",
      "House McMaster 0.0\n",
      "House Is 0.0\n",
      "House Departing 0.0\n",
      "Denies The 0.0\n",
      "Denies White 0.0\n",
      "Denies House 0.0\n",
      "Denies Denies 1.0\n",
      "Denies National 0.0\n",
      "Denies Security 0.0\n",
      "Denies Adviser 0.0\n",
      "Denies H.R. 0.0\n",
      "Denies McMaster 0.0\n",
      "Denies Is 0.0\n",
      "Denies Departing 0.0\n",
      "National The 0.0\n",
      "National White 0.0\n",
      "National House 0.0\n",
      "National Denies 0.0\n",
      "National National 1.0\n",
      "National Security 0.0\n",
      "National Adviser 0.0\n",
      "National H.R. 0.0\n",
      "National McMaster 0.0\n",
      "National Is 0.0\n",
      "National Departing 0.0\n",
      "Security The 0.0\n",
      "Security White 0.0\n",
      "Security House 0.0\n",
      "Security Denies 0.0\n",
      "Security National 0.0\n",
      "Security Security 1.0\n",
      "Security Adviser 0.0\n",
      "Security H.R. 0.0\n",
      "Security McMaster 0.0\n",
      "Security Is 0.0\n",
      "Security Departing 0.0\n",
      "Adviser The 0.0\n",
      "Adviser White 0.0\n",
      "Adviser House 0.0\n",
      "Adviser Denies 0.0\n",
      "Adviser National 0.0\n",
      "Adviser Security 0.0\n",
      "Adviser Adviser 1.0\n",
      "Adviser H.R. 0.0\n",
      "Adviser McMaster 0.0\n",
      "Adviser Is 0.0\n",
      "Adviser Departing 0.0\n",
      "H.R. The 0.0\n",
      "H.R. White 0.0\n",
      "H.R. House 0.0\n",
      "H.R. Denies 0.0\n",
      "H.R. National 0.0\n",
      "H.R. Security 0.0\n",
      "H.R. Adviser 0.0\n",
      "H.R. H.R. 1.0\n",
      "H.R. McMaster 0.0\n",
      "H.R. Is 0.0\n",
      "H.R. Departing 0.0\n",
      "McMaster The 0.0\n",
      "McMaster White 0.0\n",
      "McMaster House 0.0\n",
      "McMaster Denies 0.0\n",
      "McMaster National 0.0\n",
      "McMaster Security 0.0\n",
      "McMaster Adviser 0.0\n",
      "McMaster H.R. 0.0\n",
      "McMaster McMaster 1.0\n",
      "McMaster Is 0.0\n",
      "McMaster Departing 0.0\n",
      "Is The 0.0\n",
      "Is White 0.0\n",
      "Is House 0.0\n",
      "Is Denies 0.0\n",
      "Is National 0.0\n",
      "Is Security 0.0\n",
      "Is Adviser 0.0\n",
      "Is H.R. 0.0\n",
      "Is McMaster 0.0\n",
      "Is Is 1.0\n",
      "Is Departing 0.0\n",
      "Departing The 0.0\n",
      "Departing White 0.0\n",
      "Departing House 0.0\n",
      "Departing Denies 0.0\n",
      "Departing National 0.0\n",
      "Departing Security 0.0\n",
      "Departing Adviser 0.0\n",
      "Departing H.R. 0.0\n",
      "Departing McMaster 0.0\n",
      "Departing Is 0.0\n",
      "Departing Departing 1.0\n"
     ]
    }
   ],
   "source": [
    "#Ref: https://spacy.io/usage/examples\n",
    "# load pre-trained vectors and \n",
    "def vec(vectors_loc, lang=None):\n",
    "    if lang is None:\n",
    "        nlp = Language()\n",
    "    else:\n",
    "        nlp = spacy.blank(lang)\n",
    "    with open(vectors_loc, 'rb') as file_:\n",
    "        header = file_.readline()\n",
    "        nr_row, nr_dim = header.split()\n",
    "        nlp.vocab.reset_vectors(width=int(nr_dim))\n",
    "        for line in file_:\n",
    "            line = line.rstrip().decode('utf8')\n",
    "            pieces = line.rsplit(' ', int(nr_dim))\n",
    "            word = pieces[0]\n",
    "            vector = numpy.asarray([float(v) for v in pieces[1:]], dtype='f')\n",
    "            nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab\n",
    "    # test the vectors and similarity\n",
    "    tokens = nlp(text[3])\n",
    "    for token1 in tokens:\n",
    "        for token2 in tokens:\n",
    "            print(token1 , token2, token1.similarity(token2))\n",
    "vec(\"./model_filmpolitics.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "Code taken from respective sites has been mention above codes, some provided by Prof. Damir Cavar\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "https://pypi.python.org/pypi/fasttext\n",
    "\n",
    "https://spacy.io/usage/\n",
    "\n",
    "Discussion with Dheeraj Singh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
