{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. POS Tagging using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import plot_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = treebank.tagged_sents()\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 1161192\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.brown.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2739\n",
      "1175\n"
     ]
    }
   ],
   "source": [
    "split = int(len(tagged_sentences)*.7)\n",
    "training_sentences = tagged_sentences[:split]\n",
    "test_sentences = tagged_sentences[split:]\n",
    "\n",
    "print(len(training_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.0\n",
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t0.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t0.0\n",
      "  (0, 7)\t0.0\n",
      "  (0, 4125)\t1.0\n",
      "  (0, 12011)\t1.0\n",
      "  (0, 12493)\t1.0\n",
      "  (0, 14470)\t1.0\n",
      "  (0, 16147)\t1.0\n",
      "  (0, 28603)\t1.0\n",
      "  (0, 29206)\t1.0\n",
      "  (0, 31509)\t1.0\n",
      "  (0, 35593)\t1.0\n",
      "  (1, 0)\t0.0\n",
      "  (1, 1)\t0.0\n",
      "  (1, 2)\t0.0\n",
      "  (1, 3)\t0.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 5)\t0.0\n",
      "  (1, 6)\t0.0\n",
      "  (1, 7)\t0.0\n",
      "  :\t:\n",
      "  (71055, 11981)\t1.0\n",
      "  (71055, 12081)\t1.0\n",
      "  (71055, 12855)\t1.0\n",
      "  (71055, 27718)\t1.0\n",
      "  (71055, 28559)\t1.0\n",
      "  (71055, 28656)\t1.0\n",
      "  (71055, 29418)\t1.0\n",
      "  (71055, 32292)\t1.0\n",
      "  (71056, 0)\t0.0\n",
      "  (71056, 1)\t0.0\n",
      "  (71056, 2)\t1.0\n",
      "  (71056, 3)\t1.0\n",
      "  (71056, 4)\t1.0\n",
      "  (71056, 5)\t0.0\n",
      "  (71056, 6)\t1.0\n",
      "  (71056, 7)\t0.0\n",
      "  (71056, 8)\t1.0\n",
      "  (71056, 11977)\t1.0\n",
      "  (71056, 12054)\t1.0\n",
      "  (71056, 12816)\t1.0\n",
      "  (71056, 16614)\t1.0\n",
      "  (71056, 28555)\t1.0\n",
      "  (71056, 28633)\t1.0\n",
      "  (71056, 29296)\t1.0\n",
      "  (71056, 31832)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X, y = get_dataset(training_sentences)\n",
    "X_test, y_test = get_dataset(test_sentences)\n",
    "dict_vectorize = DictVectorizer()\n",
    "dict_vectorize.fit(X + X_test)\n",
    "X = dict_vectorize.transform(X)\n",
    "X_test = dict_vectorize.transform(X_test)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 21  3 ... 20  7  2]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y + y_test)\n",
    "y = label_encoder.transform(y)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (71057, 44234)\n",
      "y_train shape:  (71057, 46)\n",
      "x_test shape:  (29619, 44234)\n",
      "y_test shape:  (29619, 46)\n"
     ]
    }
   ],
   "source": [
    "y = np_utils.to_categorical(y)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print('x_train shape: ', X.shape)\n",
    "print('y_train shape: ', y.shape)\n",
    "print('x_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net (3 - Layered):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56845 samples, validate on 14212 samples\n",
      "Epoch 1/5\n",
      "56845/56845 [==============================] - 189s 3ms/step - loss: 3.7825 - acc: 0.7980 - val_loss: 0.5812 - val_acc: 0.9073\n",
      "Epoch 2/5\n",
      "56845/56845 [==============================] - 181s 3ms/step - loss: 0.4442 - acc: 0.9314 - val_loss: 0.4066 - val_acc: 0.9288\n",
      "Epoch 3/5\n",
      "56845/56845 [==============================] - 207s 4ms/step - loss: 0.3156 - acc: 0.9549 - val_loss: 0.3564 - val_acc: 0.9295\n",
      "Epoch 4/5\n",
      "56845/56845 [==============================] - 240s 4ms/step - loss: 0.2541 - acc: 0.9654 - val_loss: 0.3823 - val_acc: 0.9213\n",
      "Epoch 5/5\n",
      "56845/56845 [==============================] - 262s 5ms/step - loss: 0.2239 - acc: 0.9719 - val_loss: 0.3185 - val_acc: 0.9353\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=512, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=512, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, validation_split=0.2, epochs=5, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29619/29619 [==============================] - 40s 1ms/step\n",
      "0.9411188763969074\n"
     ]
    }
   ],
   "source": [
    "test_loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Dialogue Act Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import csv\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Description:\n",
    "The data has been taken from switchboard dialogue act corpus and multiple transcripts are combined with just text and a particular class of dialogue in a single csv file.\n",
    "\n",
    "The data is taken such that currently only 3 classes i.e Greetings, Goodbye and Requests have been considered as per annotations from SwDA corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data= []\n",
    "act_tags = {'fp': 'Greet', 'fc': 'Goodbye', 'qy' : 'Request', 'qw' : 'Request', 'qo' : 'Request', 'qr' : 'Request', 'qrr': 'Request'}#greeting, goodby, and request\n",
    "corpusReader = csv.reader(open('./swda/combined.csv', newline=''), delimiter =',')\n",
    "for row in corpusReader:\n",
    "    if row[1] in act_tags:\n",
    "        training_data.append({'class': act_tags[row[1]], 'sentence': row[2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= []\n",
    "classes = []\n",
    "sentences = []\n",
    "dialogues = []\n",
    "ignore_words = ['?']\n",
    "for pattern in training_data:\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    words.extend(w)\n",
    "    dialogues.append(w)\n",
    "    sentences.append(pattern['sentence'])\n",
    "    classes.append(pattern['class'])\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bof(dialogues):\n",
    "    dataset = []\n",
    "    for dialoge in dialogues:\n",
    "        bag = []\n",
    "        sent = [stemmer.stem(w.lower()) for w in dialoge if w not in ignore_words]\n",
    "        for w in words:\n",
    "            bag.append(1) if w in sent else bag.append(0)\n",
    "        dataset.append(bag)\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset\n",
    "dataset = bof(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(classes)\n",
    "labels = label_encoder.transform(classes)\n",
    "labels = np_utils.to_categorical(labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating model (3 - layer NN model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7478 samples, validate on 1870 samples\n",
      "Epoch 1/5\n",
      "7478/7478 [==============================] - 10s 1ms/step - loss: 1.4273 - acc: 0.9188 - val_loss: 0.2193 - val_acc: 0.9428\n",
      "Epoch 2/5\n",
      "7478/7478 [==============================] - 10s 1ms/step - loss: 0.1468 - acc: 0.9626 - val_loss: 0.1936 - val_acc: 0.9513\n",
      "Epoch 3/5\n",
      "7478/7478 [==============================] - 10s 1ms/step - loss: 0.1163 - acc: 0.9786 - val_loss: 0.1952 - val_acc: 0.9529\n",
      "Epoch 4/5\n",
      "7478/7478 [==============================] - 10s 1ms/step - loss: 0.0976 - acc: 0.9810 - val_loss: 0.2319 - val_acc: 0.9455\n",
      "Epoch 5/5\n",
      "7478/7478 [==============================] - 10s 1ms/step - loss: 0.0867 - acc: 0.9837 - val_loss: 0.1542 - val_acc: 0.9642\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=512, input_dim=dataset.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu', kernel_regularizer=l2(0.03)))\n",
    "model.add(Dense(units=labels.shape[1], activation='softmax'))\n",
    "https://techblog.cdiscount.com/part-speech-tagging-tutorial-keras-deep-learning-library/\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(dataset, labels, validation_split=0.2, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Greet' 'Request' 'Request' 'Greet' 'Request' 'Request']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "input_vectors = ['hello', 'How are you?', 'Excuse me', 'HI', 'Would you like to join?', '']\n",
    "input_vectors = [nltk.word_tokenize(w) for w in input_vectors]\n",
    "input_vectors = bof(input_vectors)\n",
    "label_prob = model.predict(input_vectors)\n",
    "label_pred = label_prob.argmax(axis=-1)\n",
    "print(label_encoder.inverse_transform(label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "1. http://www.aclweb.org/anthology/C94-1027\n",
    "2. https://techblog.cdiscount.com/part-speech-tagging-tutorial-keras-deep-learning-library/\n",
    "3. https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
